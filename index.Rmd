---
title: "Machine Learning HW Project"
author: "Noelle Foster"
date: "December 3, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
```

## Summary

Using random forest prediction, data on motion type collected using accelerometers for 6 participants doing different types of lifts can be used to predict the type of lift 99% of the time.  

*Cleanup and partitioning*

The main dataset is read in, partitioned into training and test sets, and cleaned to account for an astonishing percentage of missing values.  After removing identifying values and columns with more then 95% of their values missing, it goes from 160 variables to 53.

```{r}
fileTR<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
fileTST<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

library(caret)

data_tr<-read.csv(fileTR, na.strings=c("NA","#DIV/01",""))
data_tst<-read.csv(fileTST, na.strings=c("NA","#DIV/01",""))
data_tr$X<-NULL
data_tr$raw_timestamp_part_1<-NULL
data_tr$raw_timestamp_part_2<-NULL
data_tr$cvtd_timestamp<-NULL
data_tr$new_window<-NULL
data_tr$num_window<-NULL
data_tr$user_name<-NULL
missing_data<-apply(data_tr, 2, function(x) sum(is.na(x)))/nrow(data_tr)
data_tr<-data_tr[!(missing_data>0.95)]

set.seed(2525)
train<-createDataPartition(y=data_tr$classe, p=.6, list=FALSE)
myTrain<-data_tr[train,]
myTest<-data_tr[-train,]
```

*Model Creation*

The model, generated on the previously partitioned training set.  This step takes a significant amount of time, but is not outside the range of a typical consumer machine.

```{r}
mod_rf <- train(classe ~ ., data = myTrain, method = "rf")
cm_rf<-confusionMatrix(mod_rf)
cm_rf
``` 

*Model Testing*

Using the fully labeled testing dataset, the model is tested and a new confusion matrix generated to check its accuracy.
```{r}
pred_rf <- predict(mod_rf, myTest)
cm_rftst<-confusionMatrix(pred_rf, myTest$classe)
cm_rftst
```

<<<<<<< HEAD
*Variables of Interest*
```{r}
varImp(mod_rf)
=======
*Final Tree*
```{r}
library(rattle)
fancyRpartPlot(mod_rf$finalModel)
>>>>>>> 1155baf0244b4524c801705b106171e2f12de53c
```

*Discussion*
With the exception of the amount of time required to actually train the model, which may contribute to problems in scalability, this analysis yielded outstanding results, correctly predicting all 20 of the rows in the unlabeled test set.  Comparing the results with LDA and GBM analyses showed random forest to have the highest accuracy.  LDA in particular had a faster run time, but its 70% accuracy was far inferior.
